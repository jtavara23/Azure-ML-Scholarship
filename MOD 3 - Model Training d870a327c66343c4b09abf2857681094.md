# MOD 3 - Model Training

# Model Training

## Data Preparation

### Data importing and transformation

Data wrangling is the process of cleaning and transforming data to make it more appropriate for data analysis.

Data wrangling is an iterative process where you do some data transformation then check the results and come back to the process to make improvements.

### **Preparing the Data**

There are several **[assumptions](https://www.jmp.com/en_us/statistics-knowledge-portal/what-is-regression/simple-linear-regression-assumptions.html)** or conditions you need to keep in mind when you use the linear regression algorithm. If the raw data does not meet these assumptions, then it needs to be prepared and transformed prior to use.

- **Linear assumption:** As we've said earlier, linear regression describes variables using a *line*. So the relationship between the input variables and the output variable needs to be a linear relationship. If the raw data does not follow a linear relationship, you may be able to [transform](https://en.wikipedia.org/wiki/Data_transformation_(statistics)) your data prior to using it with the linear regression algorithm. For example, if your data has an exponential relationship, you can use *log transformation*.
- **Remove collinearity**: When two variables are **collinear**, this means they can be modeled by the same line or are at least highly *correlated*; in other words, one input variable can be accurately predicted by the other. For example, suppose we want to predict education level using the input variables `number of years studying at school`, `if an individual is male`, and `if an individual is female`. In this case, we will see collinearity—the input variable `if an individual is female` can be perfectly predicted by `if an individual is male`, thus, we can say they are highly correlated. Having highly correlated input variables will make the model less consistent, so it's important to perform a *correlation check* among input variables and remove highly correlated input variables.
- **Gaussian (normal) distribution:** Linear regression assumes that the distance between output variables and real data (called *residual*) is *normally distributed*. If this is not the case in the raw data, you will need to first transform the data so that the residual has a normal distribution.
- **Rescale data:** Linear regression is very sensitive to the distance among data points, so it's always a good idea to *normalize* or *standardize* the data.
- **Remove noise**: Linear regression is very sensitive to noise and *outliers* in the data. Outliers will significantly change the line learned, as shown in the picture below. Thus, cleaning the data is a critical step prior to applying linear regression

- The data management process, including:
    - The use of *datastores* and *datasets*

        **Datastores** offer a layer of abstraction over the supported Azure storage services. They store all the information needed to connect to a particular storage service. Datastores provide an access mechanism that is independent of the computer resource that is used to drive a machine learning process.

        **Datasets** are resources for exploring, transforming, and managing data in Azure ML. A dataset is essentially a reference that points to the data in storage. It is used to get specific data files in the datastores.

        **The steps of the data access workflow are:**

        1. **Create a datastore** so that you can access storage services in Azure.

            ![MOD%203%20-%20Model%20Training%20d870a327c66343c4b09abf2857681094/Untitled.png](MOD%203%20-%20Model%20Training%20d870a327c66343c4b09abf2857681094/Untitled.png)

        2. **Create a dataset**, which you will subsequently use for model training in your machine learning experiment.

            It allows you to do:

            - Have a single copy of some data in your storage, but reference it multiple times—so that you don't need to create multiple copies each time you need that data available. They are NOT actual copies, just references that point the data in your storage service.
            - Access data during model training without specifying connection strings or data paths.
            - More easily share data and collaborate with other users.
            - Bookmark the state of your data by using dataset versioning:
                - New data is available for retraining.
                - When you are applying different approaches to data preparation or feature engineering.

            Types: 

            - The **Tabular Dataset**, which represents data in a tabular format created by parsing the provided file or list of files.
            - The **Web URL (File Dataset)**, which references single or multiple files in datastores or from public URLs.
        3. **Create a dataset monitor** to detect issues in the data, such as **data drift.**

            ![MOD%203%20-%20Model%20Training%20d870a327c66343c4b09abf2857681094/Untitled%201.png](MOD%203%20-%20Model%20Training%20d870a327c66343c4b09abf2857681094/Untitled%201.png)

            **Data Drift**

            As we mentioned earlier, **data drift** is change in the input data for a model. Over time, data drift causes degradation in the model's performance, as the input data drifts farther and farther from the data on which the model was trained.

    - Versioning

        We can have different versions of our Datasets that allow us to manipulate them in different manners. 

    - Feature engineering

        Feature engineering manipulates existing features in order to create new features, with the goal of improving model training.

        Feature engineering can be implemented in multiple places, such as at the data source or during model training.

        ![MOD%203%20-%20Model%20Training%20d870a327c66343c4b09abf2857681094/Untitled%202.png](MOD%203%20-%20Model%20Training%20d870a327c66343c4b09abf2857681094/Untitled%202.png)

    - Feature Selection

        Commonly used techniques are:

        - PCA (Principal Component Analysis): A linear dimensionality reduction technique based mostly on exact mathematical calculations.
        - t-SNE (t-Distributed Stochastic Neighboring Entities): A dimensionality reduction technique based on a probabilistic approach; useful for the visualization of multidimensional data.
        - Feature embedding: Encodes a larger number of features into a smaller number of "super-features."

        Azure ML prebuilt modules:

        - Filter-based feature selection: identify columns in the input dataset that have the greatest predictive power
        - Permutation feature importance: determine the best features to use by computing the feature importance scores

    [Lab - Engineer and Select Features](https://www.notion.so/Lab-Engineer-and-Select-Features-06f2372b41cf4c6aacd780c3fb3c4825)

## Training Phase

### **Parameters and Hyperparameters**

When we train a model, a large part of the process involves learning the values of the *parameters* of the model. For example, earlier we looked at the general form for linear regression:

> y = B_0 + B_1*x_1 + B_2*x_2 + B_3*x_3 ... + B_n *x_ny=B0​+B1​∗x1​+B2​∗x2​+B3​∗x3​...+Bn​∗xn​

The coefficients in this equation, B_0 … B_n*B*0​…*Bn*​, determine the intercept and slope of the regression line. When training a linear regression model, we use the training data to figure out what the value of these *parameters* should be. Thus, we can say that *a major goal of model training is to learn the values of the model parameters.*

In contrast, some model parameters are *not* learned from the data. These are called **hyperparameters** and their values are set before training. Here are some examples of hyperparameters:

- The number of layers in a deep neural network
- The number of clusters (such as in a k-means clustering algorithm)
- The learning rate of the model

We must choose some values for these hyperparameters, but we do not necessarily know what the best values will be prior to training. Because of this, a common approach is to take a best guess, train the model, and then tune adjust or *tune* the hyperparameters based on the model's performance.

### **Splitting the Data**

As mentioned in the video, we typically want to split our data into three parts:

- **Training data**
- **Validation data**
- **Test data**

We use the **training data** to learn the values for the *parameters*. Then, we check the model's performance on the **validation data** and *tune* the hyperparameters until the model performs well with the validation data. For instance, perhaps we need to have more or fewer layers in our neural network. We can adjust this hyperparameter and then test the model on the validation data once again to see if its performance has improved.

## Evaluation Metrics

- Accuracy, precision, recall , F1 score

    ![MOD%203%20-%20Model%20Training%20d870a327c66343c4b09abf2857681094/Untitled%203.png](MOD%203%20-%20Model%20Training%20d870a327c66343c4b09abf2857681094/Untitled%203.png)

- ROC (Receiver Operating Characteristics)

    ![MOD%203%20-%20Model%20Training%20d870a327c66343c4b09abf2857681094/Untitled%204.png](MOD%203%20-%20Model%20Training%20d870a327c66343c4b09abf2857681094/Untitled%204.png)

- AUC ( Area Under the Curve)

    An AUC of 0.5 indicates random guessing, while an AUC of 1.0 indicates perfect classification.

- Regression Metrics
    - R-Squared, RMSE(Root Mean SE), MAE(Mean Absolute Error) , Spearman correlation

        ![MOD%203%20-%20Model%20Training%20d870a327c66343c4b09abf2857681094/Untitled%205.png](MOD%203%20-%20Model%20Training%20d870a327c66343c4b09abf2857681094/Untitled%205.png)

### Lab - Train and Evaluate Model

[https://youtu.be/P8XkYitGtXE](https://youtu.be/P8XkYitGtXE)

## Ensemble and Automated Machine Learning

### Ensemble:

Combines multiple learning models to produce one predictive model.

There are three main types of ensemble algorithms: 

**Bagging** or **bootstrap aggregation**

- Helps reduce overfitting for models that tend to have high variance (such as *decision trees*)
- Uses random subsampling of the training data to produce a *bag* of trained models.
- The resulting trained models are homogeneous.
- The final prediction is an average prediction from individual models

**Boosting**

- Helps reduce bias for models.
- In contrast to bagging, boosting uses the same input data to train multiple models using different hyperparameters.
- Boosting trains model in *sequence* by training weak learners one by one, with each new learner correcting errors from previous learners
- The final predictions are a *weighted* average from the individual models.

**Stacking**

- Trains a large number of completely different (heterogeneous) models
- Combines the outputs of the individual models into a meta-model that yields more accurate predictions

### Train a Two Class Boosted Decision Tree

[https://youtu.be/AhLfTUincPQ](https://youtu.be/AhLfTUincPQ)

### Automated machine learning:

Like the name suggests, automates many of the iterative, time-consuming, tasks involved in model development (such as selecting the best features, scaling features optimally, choosing the best algorithms, and tuning hyperparameters). 

Automated ML allows data scientists, analysts, and developers to build models with greater scale, efficiency, and productivity—all while sustaining model quality.

### Train a Simple Classifier with Automated ML

[https://youtu.be/GQA96QHGfyE](https://youtu.be/GQA96QHGfyE)